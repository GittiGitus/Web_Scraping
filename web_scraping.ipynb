{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupyter notebook, the Deutsche Welle website is scraped using the package BeautifulSoup.\\\n",
    "The scraping is mainly achieved with three functions.\\\n",
    "First, the overview page of Deutsche Welle is scraped using the function **scrape_dw_overview_page()**. The function takes a start and an end date and returns a DataFrame with the urls of all articles in between the two dates.\\\n",
    "Secondly, more detailed information of each article is scraped with the function **get_detailed_text()**. This function uses the returned DataFrame of the function scrape_dw_overview_page() and returns a DataFrame that includes all the required information about each article in the input DataFrame.\\\n",
    "Finally, the function **scrape_dw()** combines the first two functions and returns the required DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_dw_overview_page(start_date, end_date):\n",
    "    try:\n",
    "        # Define the URL of the Page, we want to Scrape and add the start and end dates\n",
    "        url = f'https://www.dw.com/search/?languageCode=en&contentType=ARTICLE&searchNavigationId=9097-30688&from={start_date}&to={end_date}&sort=DATE&resultsCounter=10'\n",
    "\n",
    "        # Request the html_code of the url defined above\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "\n",
    "            # Get the html_code\n",
    "            html = response.text\n",
    "\n",
    "            # Parse the html_code\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "            # Get all articles in soup\n",
    "            document_div = soup.find('div', attrs={'class': 'searchResults col4'})\n",
    "            all_search_results = document_div.find_all('div', attrs={'class': 'searchResult'})\n",
    "\n",
    "            # Set maximum elements\n",
    "            max_elements = int(document_div.find('span', attrs={'class': 'hits all'}).text.strip())\n",
    "\n",
    "            # Define the url again and adding max_elements to the request url\n",
    "            url = f'https://www.dw.com/search/?languageCode=en&contentType=ARTICLE&searchNavigationId=9097-30688&from={start_date}&to={end_date}&sort=DATE&resultsCounter={max_elements}'\n",
    "            html = requests.get(url).text\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "            # Get all articles in soup\n",
    "            document_div = soup.find('div', attrs={'class': 'searchResults col4'})\n",
    "            all_search_results = document_div.find_all('div', attrs={'class': 'searchResult'})\n",
    "\n",
    "            # Define a empty DataFrame\n",
    "            df_scrapped = pd.DataFrame(columns=[\"url\"])\n",
    "\n",
    "            # Get url for all articles in all_search_results,\n",
    "            for i in range(0, int(max_elements)):\n",
    "                # Build the URL of a specific article\n",
    "                url = f\"https://www.dw.com{all_search_results[i].find('a').get('href')}\"\n",
    "\n",
    "                # Create a DataFrame with the current URL and append it to df_scrapped\n",
    "                current_row = pd.DataFrame({'url': [url]})\n",
    "                df_scrapped = pd.concat([df_scrapped, current_row], ignore_index=True)\n",
    "\n",
    "                time.sleep(1)\n",
    "\n",
    "            return df_scrapped\n",
    "        else: # If the request was not successful\n",
    "            print(f\"Failed to retrieve data from {url}. Status code: {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e: # If an error occurred\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detailed_text(df):\n",
    "\n",
    "    # Add empty columns to the DataFrame\n",
    "    df[\"autor\"] = \"\"\n",
    "    df[\"date\"] = \"\"\n",
    "    df[\"category\"]= \"\"\n",
    "    df[\"region\"] = \"\"  \n",
    "    df[\"title\"] = \"\"\n",
    "    df[\"summary\"] = \"\"\n",
    "    df[\"text\"] = \"\"\n",
    "    df[\"subheadings\"] = \"\"\n",
    "    df[\"related_topics\"] = \"\"\n",
    "    \n",
    "    # Ensure that soup is empty\n",
    "    soup = \"\" \n",
    "\n",
    "    # Loop through all articles\n",
    "    for id in range(0, len(df)):\n",
    "        print(f\"\\nScraping article {id} of {len(df)}\")\n",
    "\n",
    "        # Get the url of the i-th article\n",
    "        url = df.iloc[id].url\n",
    "\n",
    "        try: \n",
    "            # Request the url\n",
    "            response = requests.get(url)\n",
    "\n",
    "        except:\n",
    "            print(f\"No article found for the url {url}\\n\")\n",
    "\n",
    "        # If request was successfully \n",
    "        if response.status_code == 200: \n",
    "\n",
    "            # Get the html code\n",
    "            html = response.text\n",
    "            \n",
    "            # Create soup by parse the html code\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "             \n",
    "            # Add url to DataFrame\n",
    "            df[\"url\"][id] = url\n",
    "\n",
    "            # Try to get author\n",
    "            try:\n",
    "                # Find the a element with the specified class\n",
    "                a_author = soup.find('a', class_='author-link')\n",
    "                # Extract the author's name from the span element and add it to the DataFrame\n",
    "                df[\"autor\"][id]   = a_author.find('span', class_='sc-ezGUZh').text.strip()\n",
    "\n",
    "            except:\n",
    "                print(f\"No author found for article {df.iloc[id].title}\\n{url}\\n\")\n",
    "                df[\"autor\"][id]  = None\n",
    "\n",
    "            # Try to get the date\n",
    "            try:\n",
    "                # Find the span element with the specified class directly\n",
    "                span_publication = soup.find('span', class_='publication')\n",
    "                time_element = span_publication.find('time')\n",
    "\n",
    "                # Extract the date from time element \n",
    "                date = time_element.text.strip()\n",
    "\n",
    "                # Format date like'%Y-%m-%d'\n",
    "                date = datetime.datetime.strptime(date,'%m/%d/%Y').strftime('%Y-%m-%d')\n",
    "\n",
    "                # Add the date to the DataFrame\n",
    "                df[\"date\"][id]  = date\n",
    "\n",
    "            except:\n",
    "                print(f\"No date found for article {df.iloc[id].title}\\n{url}\\n\")\n",
    "                df[\"date\"][id]  = None \n",
    "\n",
    "            # Try to get category\n",
    "            try:\n",
    "                # Find the div element with the specified class\n",
    "                div_kicker = soup.find('div', class_='kicker')\n",
    "        \n",
    "                # Find the span elements within the div\n",
    "                span_elements = div_kicker.find_all('span')\n",
    "\n",
    "                # Extract the category \n",
    "                df[\"category\"][id] = span_elements[0].text\n",
    "\n",
    "            except:\n",
    "                print(f\"No category found for article {df.iloc[id].title}\\n{url}\\n\")\n",
    "                df[\"category\"][id] = None\n",
    "\n",
    "            # Try to get region\n",
    "            try:\n",
    "                # Extract the region \n",
    "                df[\"region\"][id] = span_elements[1].text\n",
    "\n",
    "            except:\n",
    "                print(f\"No region found for article {df.iloc[id].title}\\n{url}\\n\")\n",
    "                df[\"region\"][id] = None\n",
    "\n",
    "            # Try to get title\n",
    "            try:\n",
    "                # Find the h1 element with the specified class\n",
    "                h1 = soup.find('h1', class_='sc-HjNCl wdGIM sc-iuWDFx cFfUdQ')\n",
    "\n",
    "                # Extract the title from the h1 element and add it to the DataFrame\n",
    "                df[\"title\"][id] = h1.text.strip()\n",
    "                \n",
    "            except:\n",
    "                print(f\"No title found for article {df.iloc[id].title}\\n{url}\\n\")\n",
    "                df[\"title\"][id] = None\n",
    "\n",
    "            # Try to get summary\n",
    "            try:\n",
    "                # get summary via css selector\n",
    "                p_summary = soup.select_one(\"p.sc-ezGUZh\")\n",
    "                \n",
    "                # Extract the summary from the p element and add it to the DataFrame\n",
    "                df[\"summary\"][id] = p_summary.text.strip()\n",
    "\n",
    "            except:\n",
    "                print(f\"No summary found for article {df.iloc[id].title}\\n{url}\\n\")\n",
    "                df[\"summary\"][id] = None\n",
    "\n",
    "            # Try to get text\n",
    "            try:\n",
    "                # Find the main content element\n",
    "                main_content = soup.find('div', class_='sc-ezGUZh sc-kMbQoj llLYdd itboQC sc-czCoYo iBsIdr rich-text has-italic')\n",
    "                \n",
    "                # Get all p elements within the main content\n",
    "                p_elements = main_content.find_all('p')\n",
    "\n",
    "                # Extract the text of the p elements and combine them to one string\n",
    "                text = \" \".join([p.text.replace('\\xa0', ' ') for p in p_elements])\n",
    "\n",
    "                # Add \" to the beginning and end of the string\n",
    "                text = '\"' + text + '\"'\n",
    "                \n",
    "                # Add the text to the DataFrame\n",
    "                df[\"text\"][id] = text\n",
    "\n",
    "            except:\n",
    "                print(f\"No text found for article {df.iloc[id].title}\\n{url}\\n\")\n",
    "                df[\"text\"][id] = None\n",
    "\n",
    "            # Try to get subheadings\n",
    "            try:\n",
    "                # Find the main content element\n",
    "                main_content = soup.find('div', class_='sc-ezGUZh sc-kMbQoj llLYdd itboQC sc-czCoYo iBsIdr rich-text has-italic')\n",
    "                \n",
    "                # Get the subheadings via h2 within the main content\n",
    "                h2_subheadings = main_content.find_all('h2')\n",
    "\n",
    "                # Extract the text of the subheadings and remove the \\xa0\n",
    "                subheadings = [h2.text.replace('\\xa0', ' ') for h2 in h2_subheadings]\n",
    "\n",
    "                # Assign the subheadings to the DataFrame\n",
    "                df[\"subheadings\"][id] = subheadings\n",
    "\n",
    "            except:\n",
    "                print(f\"No subheadings found for article {df.iloc[id].title}\\n{url}\\n\")\n",
    "                df[\"subheadings\"][id] = None\n",
    "\n",
    "            # Try to get related topics\n",
    "            try:\n",
    "                # Find the aside element with the specified class\n",
    "                links = soup.find('aside', class_='link-wrapper').find_all('a')\n",
    "                \n",
    "                # Extract the text of the links\n",
    "                text_list = [link.text for link in links]\n",
    "\n",
    "                # Assign the related topics to the DataFrame\n",
    "                df[\"related_topics\"][id] = text_list\n",
    "\n",
    "            except:\n",
    "                print(f\"No related topics found for article {df.iloc[id].title}\\n{url}\\n\")\n",
    "                df[\"related_topics\"][id] = None\n",
    "                \n",
    "            # Wait for a second \n",
    "            time.sleep(1)\n",
    "\n",
    "    # Return the DataFrame\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_transform(start, end): \n",
    "    '''\n",
    "    Function to transform the date format from YYYY-MM-DD to DD.MM.YYYY\n",
    "    \n",
    "    Args: \n",
    "        start and end date in the format YYYY-MM-DD\n",
    "    Return: \n",
    "        start and end date in the format DD.MM.YYYY\n",
    "    '''\n",
    "    start_date = datetime.datetime.strptime(start, '%Y-%m-%d')\n",
    "    end_date = datetime.datetime.strptime(end, '%Y-%m-%d')\n",
    "    start_date_t = datetime.datetime.strftime(start_date, '%d.%m.%Y')\n",
    "    end_date_t = datetime.datetime.strftime(end_date, '%d.%m.%Y')\n",
    "    return start_date_t, end_date_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_dw(start, end):\n",
    "    '''\n",
    "    Function to scrape articles from DW website\n",
    "\n",
    "    Args:\n",
    "        start (str): start date in format yyyy-mm-dd\n",
    "        end (str): end date in format yyyy-mm-dd\n",
    "\n",
    "    Returns: \n",
    "        df (pd.DataFrame): DataFrame with scraped articles\n",
    "    '''\n",
    "    # Transform date format from yyyy-mm-dd to dd.mm.yyyy\n",
    "    start_date, end_date = date_transform(start, end)\n",
    "    \n",
    "    # Check if start_date is before end_date\n",
    "    if start_date > end_date:\n",
    "        # switch start_date and end_date\n",
    "        end_date, start_date = start_date, end_date\n",
    "    \n",
    "    # Get overview page\n",
    "    df_overview = scrape_dw_overview_page(start_date=start_date, end_date=end_date)\n",
    "    \n",
    "    # Check if scraping overview page was successful\n",
    "    if df_overview is not None:\n",
    "        # Get detailed text\n",
    "        df = get_detailed_text(df_overview)\n",
    "        return df\n",
    "    else:\n",
    "        print('No articles found.')\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domonstration that the function scrape_dw() works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping article 0 of 27\n",
      "\n",
      "Scraping article 1 of 27\n",
      "No author found for article \n",
      "https://www.dw.com/en/scholz-and-macron-convene-for-strategic-retreat-in-hamburg/a-67047196\n",
      "\n",
      "\n",
      "Scraping article 2 of 27\n",
      "No author found for article \n",
      "https://www.dw.com/en/xi-to-senators-us-china-ties-impact-destiny-of-mankind/a-67046529\n",
      "\n",
      "\n",
      "Scraping article 3 of 27\n",
      "No author found for article \n",
      "https://www.dw.com/en/russia-uses-hamas-attacks-on-israel-for-domestic-propaganda/a-67042395\n",
      "\n",
      "\n",
      "Scraping article 4 of 27\n",
      "\n",
      "Scraping article 5 of 27\n",
      "\n",
      "Scraping article 6 of 27\n",
      "No author found for article \n",
      "https://www.dw.com/en/liberia-election-george-weah-seeks-reelection/a-67040415\n",
      "\n",
      "No related topics found for article Liberia election: George Weah seeks reelection\n",
      "https://www.dw.com/en/liberia-election-george-weah-seeks-reelection/a-67040415\n",
      "\n",
      "\n",
      "Scraping article 7 of 27\n",
      "\n",
      "Scraping article 8 of 27\n",
      "No author found for article \n",
      "https://www.dw.com/en/afghanistan-earthquake-aid-agencies-appeal-for-help/a-67041833\n",
      "\n",
      "\n",
      "Scraping article 9 of 27\n",
      "No author found for article \n",
      "https://www.dw.com/en/israel-what-was-tel-aviv-like-during-the-hamas-attacks/a-67037460\n",
      "\n",
      "\n",
      "Scraping article 10 of 27\n",
      "No author found for article \n",
      "https://www.dw.com/en/germany-scholz-coalition-battered-in-bavaria-hesse-elections/a-67037370\n",
      "\n",
      "\n",
      "Scraping article 11 of 27\n",
      "No author found for article \n",
      "https://www.dw.com/en/uk-court-to-hear-government-appeal-on-rwanda-migrant-plan/a-67037594\n",
      "\n",
      "\n",
      "Scraping article 12 of 27\n",
      "\n",
      "Scraping article 13 of 27\n",
      "No author found for article \n",
      "https://www.dw.com/en/nobel-prize-claudia-goldin-wins-2023-award-for-economics/a-67021919\n",
      "\n",
      "No related topics found for article Nobel Prize: Claudia Goldin wins 2023 award for economics\n",
      "https://www.dw.com/en/nobel-prize-claudia-goldin-wins-2023-award-for-economics/a-67021919\n",
      "\n",
      "\n",
      "Scraping article 14 of 27\n",
      "\n",
      "Scraping article 15 of 27\n",
      "No author found for article \n",
      "https://www.dw.com/en/un-human-rights-council-puts-abusers-in-guardian-role/a-66801110\n",
      "\n",
      "\n",
      "Scraping article 16 of 27\n",
      "No author found for article \n",
      "https://www.dw.com/en/typhoon-koinu-brings-floods-to-hong-kong/a-67036495\n",
      "\n",
      "\n",
      "Scraping article 17 of 27\n",
      "No author found for article \n",
      "https://www.dw.com/en/former-eastern-blocs-resistance-in-the-spotlight-at-dok-leipzig/a-66934530\n",
      "\n",
      "\n",
      "Scraping article 18 of 27\n",
      "No author found for article \n",
      "https://www.dw.com/en/what-to-know-about-the-gaza-strip/a-67034652\n",
      "\n",
      "\n",
      "Scraping article 19 of 27\n",
      "\n",
      "Scraping article 20 of 27\n",
      "No author found for article \n",
      "https://www.dw.com/en/kenyas-kelvin-kiptum-smashes-mens-marathon-world-record/a-67033985\n",
      "\n",
      "\n",
      "Scraping article 21 of 27\n",
      "No author found for article \n",
      "https://www.dw.com/en/kelvin-kiptum-sets-new-marathon-world-record-in-chicago/a-67033834\n",
      "\n",
      "\n",
      "Scraping article 22 of 27\n",
      "\n",
      "Scraping article 23 of 27\n",
      "No author found for article \n",
      "https://www.dw.com/en/spain-protest-against-catalan-amnesty-deal-draws-huge-crowd/a-67033403\n",
      "\n",
      "\n",
      "Scraping article 24 of 27\n",
      "No author found for article \n",
      "https://www.dw.com/en/what-to-know-about-hamas/a-67029343\n",
      "\n",
      "\n",
      "Scraping article 25 of 27\n",
      "No author found for article \n",
      "https://www.dw.com/en/afghanistan-earthquakes-kill-more-than-2000-taliban-says/a-67031328\n",
      "\n",
      "\n",
      "Scraping article 26 of 27\n",
      "No author found for article \n",
      "https://www.dw.com/en/california-governor-scraps-bill-against-caste-discrimination/a-67030645\n",
      "\n",
      "No related topics found for article California governor vetoes caste discrimination bill\n",
      "https://www.dw.com/en/california-governor-scraps-bill-against-caste-discrimination/a-67030645\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call scrape_dw \n",
    "df = scrape_dw( start='2023-10-08', end='2023-10-09')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the shape of the DataFrame\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the first 5 rows of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>autor</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "      <th>region</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>text</th>\n",
       "      <th>subheadings</th>\n",
       "      <th>related_topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.dw.com/en/hamas-attacks-on-israel-...</td>\n",
       "      <td>Lisa Hänel</td>\n",
       "      <td>2023-10-09</td>\n",
       "      <td>Conflicts</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Hamas attacks on Israel triggers debate in Ger...</td>\n",
       "      <td>In the wake of the terrorist attack by Islamis...</td>\n",
       "      <td>\"It began with a tweet by the German Minister ...</td>\n",
       "      <td>[Diverse Muslim community, Central Council of ...</td>\n",
       "      <td>[Rhine River, Robert Habeck, Poverty in German...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.dw.com/en/scholz-and-macron-conven...</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-10-09</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Scholz and Macron convene for 'strategic' retr...</td>\n",
       "      <td>Chancellor Olaf Scholz welcomed President Emma...</td>\n",
       "      <td>\"German Chancellor Olaf Scholz on Monday welco...</td>\n",
       "      <td>[Franco-German ties 'more important than ever'...</td>\n",
       "      <td>[Emmanuel Macron, French elections, Rhine Rive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.dw.com/en/xi-to-senators-us-china-...</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-10-09</td>\n",
       "      <td>Politics</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>Xi to senators: US-China ties impact 'destiny ...</td>\n",
       "      <td>A US delegation met with Chinese President Xi ...</td>\n",
       "      <td>\"Chinese President Xi Jinping on Monday told a...</td>\n",
       "      <td>[Managing tensions in the South China Sea, Man...</td>\n",
       "      <td>[Uyghur community, Pentagon, Washington, White...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.dw.com/en/russia-uses-hamas-attack...</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-10-09</td>\n",
       "      <td>Conflicts</td>\n",
       "      <td>Russian Federation</td>\n",
       "      <td>Russia uses Hamas attacks on Israel for domest...</td>\n",
       "      <td>Moscow, which has enjoyed good ties with Israe...</td>\n",
       "      <td>\"The flag at the Israeli embassy in Moscow is ...</td>\n",
       "      <td>[Russia condemns escalation, Israel condemns R...</td>\n",
       "      <td>[Dmitry Medvedev, BRICS, Black Sea, Russia, Ru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.dw.com/en/former-israeli-football-...</td>\n",
       "      <td>Stefan Nestler</td>\n",
       "      <td>2023-10-09</td>\n",
       "      <td>Soccer</td>\n",
       "      <td>Israel</td>\n",
       "      <td>Former Israeli football star Lior Asulin murde...</td>\n",
       "      <td>Israeli football fans are mourning the loss of...</td>\n",
       "      <td>\"A day after he had turned 43, Lior Asulin was...</td>\n",
       "      <td>[History maker, Nomadic career]</td>\n",
       "      <td>[Terrorism, Bayern Munich, Israel, Hamas]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url           autor  \\\n",
       "0  https://www.dw.com/en/hamas-attacks-on-israel-...      Lisa Hänel   \n",
       "1  https://www.dw.com/en/scholz-and-macron-conven...            None   \n",
       "2  https://www.dw.com/en/xi-to-senators-us-china-...            None   \n",
       "3  https://www.dw.com/en/russia-uses-hamas-attack...            None   \n",
       "4  https://www.dw.com/en/former-israeli-football-...  Stefan Nestler   \n",
       "\n",
       "         date   category                    region  \\\n",
       "0  2023-10-09  Conflicts                   Germany   \n",
       "1  2023-10-09   Politics                   Germany   \n",
       "2  2023-10-09   Politics  United States of America   \n",
       "3  2023-10-09  Conflicts        Russian Federation   \n",
       "4  2023-10-09     Soccer                    Israel   \n",
       "\n",
       "                                               title  \\\n",
       "0  Hamas attacks on Israel triggers debate in Ger...   \n",
       "1  Scholz and Macron convene for 'strategic' retr...   \n",
       "2  Xi to senators: US-China ties impact 'destiny ...   \n",
       "3  Russia uses Hamas attacks on Israel for domest...   \n",
       "4  Former Israeli football star Lior Asulin murde...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  In the wake of the terrorist attack by Islamis...   \n",
       "1  Chancellor Olaf Scholz welcomed President Emma...   \n",
       "2  A US delegation met with Chinese President Xi ...   \n",
       "3  Moscow, which has enjoyed good ties with Israe...   \n",
       "4  Israeli football fans are mourning the loss of...   \n",
       "\n",
       "                                                text  \\\n",
       "0  \"It began with a tweet by the German Minister ...   \n",
       "1  \"German Chancellor Olaf Scholz on Monday welco...   \n",
       "2  \"Chinese President Xi Jinping on Monday told a...   \n",
       "3  \"The flag at the Israeli embassy in Moscow is ...   \n",
       "4  \"A day after he had turned 43, Lior Asulin was...   \n",
       "\n",
       "                                         subheadings  \\\n",
       "0  [Diverse Muslim community, Central Council of ...   \n",
       "1  [Franco-German ties 'more important than ever'...   \n",
       "2  [Managing tensions in the South China Sea, Man...   \n",
       "3  [Russia condemns escalation, Israel condemns R...   \n",
       "4                    [History maker, Nomadic career]   \n",
       "\n",
       "                                      related_topics  \n",
       "0  [Rhine River, Robert Habeck, Poverty in German...  \n",
       "1  [Emmanuel Macron, French elections, Rhine Rive...  \n",
       "2  [Uyghur community, Pentagon, Washington, White...  \n",
       "3  [Dmitry Medvedev, BRICS, Black Sea, Russia, Ru...  \n",
       "4          [Terrorism, Bayern Munich, Israel, Hamas]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 5 rows of the DataFrame\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look where information is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url                0\n",
       "autor             19\n",
       "date               0\n",
       "category           0\n",
       "region             0\n",
       "title              0\n",
       "summary            0\n",
       "text               0\n",
       "subheadings        0\n",
       "related_topics     3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_sma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
